{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "# create cache from scratch\n",
    "# GutenbergCache.create(refresh=False, download=True, unpack=True, parse=True, cache=True, deleteTemp=True)\n",
    "\n",
    "cache = GutenbergCache.get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5637, 28800, 50922, 48433, 5682, 46060, 26585, 5683, 5684, 52821, 4280, 59023]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the cache.query function's kwargs expect lists, like this:\n",
    "cache.query(authors=['Kant, Immanuel'], languages=['en'])\n",
    "# i'm not sure how to make it return the titles too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenbergpy.textget import get_text_by_id, strip_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_text(author):\n",
    "    # wrapper function returning a cleaned and concatenated huge string of all of `author`'s english texts. Doesn't work very well\n",
    "    assert cache\n",
    "    book_ids = cache.query(authors=[author], languages=['en'], downloadtype=['application/plain', 'text/plain'])\n",
    "    corpus = \"\"\n",
    "    for book_id in book_ids:\n",
    "        print(\"concatenating book id \" + str(book_id) + \" ...\")\n",
    "        corpus += str(strip_headers(get_text_by_id(book_id)))\n",
    "    return corpus.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "#from nlpia.loaders import get_data\n",
    "#word_vectors = get_data('wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kant_book_ids = [4280, 5682, 5683, 5684, 46060, 48433, 50922, 52821]\n",
    "kant_book_ids_kingsmill = [5682, 5683, 5684]  # trans. Kingsmill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books_from_ids(l):\n",
    "    corpus = \"\"\n",
    "    for id in l:\n",
    "        print(\"concatenating book id \" + str(id) + \" ...\")\n",
    "        corpus += str(strip_headers(get_text_by_id(id)))\n",
    "    return str(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    r_space = re.compile(r'((\\\\n)+)|((\\\\x..)+)|((\\\\t)+)')\n",
    "    r_del = re.compile(r\"(\\\\)|'b'|\\\"b'|'b\\\"\")\n",
    "    return re.sub(r_del, '', re.sub(r_space, ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample)\n",
    "        sample_vecs = []\n",
    "        skipped_tokens = set()\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                skipped_tokens.add(token)\n",
    "                pass\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    print(\"skipped tokens:\")\n",
    "    print(skipped_tokens)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating book id 4280 ...\n",
      "concatenating book id 5682 ...\n",
      "concatenating book id 5683 ...\n",
      "concatenating book id 5684 ...\n",
      "concatenating book id 46060 ...\n",
      "concatenating book id 48433 ...\n",
      "concatenating book id 50922 ...\n",
      "concatenating book id 52821 ...\n"
     ]
    }
   ],
   "source": [
    "kant = clean_text(get_books_from_ids(kant_book_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "kant_tokens = tokenizer.tokenize(kant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following tut https://keras.io/examples/generative/lstm_character_level_text_generation/\n",
    "def train_character_model(text,   # string\n",
    "                          lower=False,  # make it lowercase\n",
    "                          print_progress=True, \n",
    "                          save_model=False, \n",
    "                          save_checkpoints=False, \n",
    "                          epochs=40, \n",
    "                          batch_size=128\n",
    "                         ):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    maxlen = 40\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i : i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.GRU(128, input_shape=(maxlen, len(chars))))\n",
    "    model.add(keras.layers.Dense(len(chars)))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    model.summary()\n",
    "    \n",
    "    if save_checkpoints:\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"checkpoints/kant_character_model_{epoch:02d}_{loss:.2f}\",\n",
    "            save_weights_only=True,\n",
    "            monitor='loss',\n",
    "            mode='min',\n",
    "        )\n",
    "    \n",
    "    if print_progress:\n",
    "        for epoch in range(epochs):\n",
    "            if save_checkpoints:\n",
    "                model.fit(x, y, batch_size=batch_size, epochs=1, callbacks=[checkpoint_cb])\n",
    "            else:\n",
    "                model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "            print()\n",
    "            print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "            start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "            for diversity in [0.2, 0.5, 1.0]:\n",
    "                print(\"...Diversity:\", diversity)\n",
    "\n",
    "                generated = \"\"\n",
    "                sentence = text[start_index : start_index + maxlen]\n",
    "                print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "                for i in range(400):\n",
    "                    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x_pred[0, t, char_indices[char]] = 1.0\n",
    "                    preds = model.predict(x_pred, verbose=0)[0]\n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_char = indices_char[next_index]\n",
    "                    sentence = sentence[1:] + next_char\n",
    "                    generated += next_char\n",
    "\n",
    "                print(\"...Generated: \", generated)\n",
    "                print()\n",
    "    else:\n",
    "        if save_checkpoints:\n",
    "            model.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=[checkpoint_cb])\n",
    "        else:\n",
    "            model.fit(x, y, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    if save_model:\n",
    "        keras.models.save_model(model, os.path.join('saved_models', 'kant_character_model'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1109853\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 128)               82944     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 86)                11094     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 86)                0         \n",
      "=================================================================\n",
      "Total params: 94,038\n",
      "Trainable params: 94,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1109853 samples\n",
      "1109853/1109853 [==============================] - 1744s 2ms/sample - loss: 1.4662\n",
      "\n",
      "Generating text after epoch: 0\n",
      "...Diversity: 0.2\n",
      "...Generating with seed: \" suppose the case of a righteous man [_e\"\n",
      "...Generated:  mpirical principles of the constitution of the condition of the condition of the condition of the condition of the conception of the condition of the condition of the condition of the condition of a principle of the condition of the considered the constitution of the conception of the condition of the condition of the condition of the condition of the condition of the condition of the condition in\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed: \" suppose the case of a righteous man [_e\"\n",
      "...Generated:  mploy_ impures imperted. The synthitions of an intuitions as a pretection of the case the conception of the case of the possible as it as the contrary the considered of the pure intelegent be the satisfical considered the categories of the condition of reason in the causal presing presents all objects of the constitutions of the objection.                                                           \n",
      "\n",
      "...Diversity: 1.0\n",
      "...Generating with seed: \" suppose the case of a righteous man [_e\"\n",
      "...Generated:  ppert. Withing therefore, as the latter to be known of opiht anal_soliveness, apporit so must presents the presing practical advent of which is be concedirent, can bo compals as reason are _quitely eveak agains: the cereal taken tfanscepreterng the putechend the of this  iftentur but must be constitute perhasly the causas, and afchementations as a must aeptant such Ari.e be always which I make not\n",
      "\n",
      "...Diversity: 1.2\n",
      "...Generating with seed: \" suppose the case of a righteous man [_e\"\n",
      "...Generated:  n _S._dut ho_ mer. Bical presents must nasure. minity aptituces; a midly of suact of naitrelfod applied transgation Arr of Noworks precinction my ancion hower practical  Equise bosableness fir my muss trakely pances, nothole suces but untur undertand edit lies alathorm; and twa purbourgation to only first, impleyion as agaiss distinctuuty, but if. TI. find I ourgislforeat however is, if is not his\n",
      "\n",
      "Train on 1109853 samples\n",
      "1109853/1109853 [==============================] - 1826s 2ms/sample - loss: 1.8839\n",
      "\n",
      "Generating text after epoch: 1\n",
      "...Diversity: 0.2\n",
      "...Generating with seed: \"losophy, it is easy to set aside the who\"\n",
      "...Generated:  sese one of ofone on of of of ine te one the se ie of of of ie the one of of of of one the one of of the con of of of of of of of ine the  on on of in f of of of ofe the tene one of ofe t of o of ofo of of ofhe ce of of of of of in ore of  exe on an the on of ine the of of ine the the on of of be expexpe ofe of of of of of of ofonsone on ouhe sone one on one ine of of of of ofon of of the teone on\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed: \"losophy, it is easy to set aside the who\"\n",
      "...Generated:  s of of ef ofese of de eoere ine ofo of thes tee ce andatis of he obje lecs Eone ese orde oneans, aste o theal the e w s the ge te one an so inge te one cone thersens in of a te oe the w teof intes ouges of he omee of pete aninis of of thed at ineve of he   one one sonese of one wse ce tes of ofon on one ale a of  ce thes o in of of  sene of I ofont ofe  anstinus ou inge sinece on soing of ofascon\n",
      "\n",
      "...Diversity: 1.0\n",
      "...Generating with seed: \"losophy, it is easy to set aside the who\"\n",
      "...Generated:  re ooece n ife ome teince  tesdgofecse woelero doms tee ist an iofo of fo olte onatan-fherl ttegd aty l icld se wo soreyte), Frule o hhenone of adhe me osing, opoo nost orlcee S thone oupo peaosteckes, are d lomece as tobeiaslos neeonealles  atatosshn fythe ofofwsome rees  astiseme of ine ane onongercise esalleenswecmeethe itionge iet tonthentatesolus fen fedect Bues of--- pe cor te o of alableden\n",
      "\n",
      "...Diversity: 1.2\n",
      "...Generating with seed: \"losophy, it is easy to set aside the who\"\n",
      "...Generated:  ingerenanssssomuIpctecne l t id asncerteaytef le) remur mbothong, jumeleltansontao oroutighy Plt ofir ecuanumra ene alsseph wetiencosmes-csg), F5o uuofoe ,  ususemo Hhe cf f por t, bueasaas--ecaceove o be inos, esg) nouone egest o exhiche ce,  of bj tgret ese tnequ ielunHocorioce ep thadpemeieatan oupe-ot, pmansecheneve imen seco eosoralls ret de.  icdurestesosin onfalo seste fbreane ct .]osasildi\n",
      "\n",
      "Train on 1109853 samples\n",
      "  32768/1109853 [..............................] - ETA: 31:12 - loss: 2.4900"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-93147110fc2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchar_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_character_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-0762f9065fe5>\u001b[0m in \u001b[0;36mtrain_character_model\u001b[0;34m(text, epochs, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating text after epoch: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "char_model = train_character_model(kant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_model(text, print_progress=True, maxlen=40, step=3, epochs=40, batch_size=128):\n",
    "    # wow this is monstrous apparently. An epoch takes an hour to run, and running this as a function crashes Jupyter.\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    unique_tokens = sorted(list(set(tokens)))\n",
    "    token_indices = dict((t, i) for i, t in enumerate(unique_tokens))\n",
    "    indices_tokens = dict((i, t) for i, t in enumerate(unique_tokens))\n",
    "    \n",
    "    sentences = []\n",
    "    next_tokens = []\n",
    "    for i in range(0, len(tokens)-maxlen, step):\n",
    "        sentences.append(tokens[i : i + maxlen])\n",
    "        next_tokens.append(tokens[i + maxlen])\n",
    "    print(\"Number of sequences:\", len(sentences))\n",
    "    \n",
    "    # i think these are one-hot encoded matrices?\n",
    "    x = np.zeros((len(sentences), maxlen, len(unique_tokens)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(unique_tokens)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, token in enumerate(sentence):\n",
    "            x[i, t, token_indices[token]] = 1\n",
    "        y[i, token_indices[next_tokens[i]]] = 1\n",
    "        \n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(maxlen, len(unique_tokens))),\n",
    "            layers.LSTM(128),\n",
    "            layers.Dense(len(unique_tokens), activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    \n",
    "    if print_progress:\n",
    "        for epoch in range(epochs):\n",
    "            model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "            print()\n",
    "            print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "            start_index = random.randint(0, len(tokens) - maxlen - 1)\n",
    "            for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "                print(\"...Diversity:\", diversity)\n",
    "\n",
    "                generated = \"\"\n",
    "                sentence = tokens[start_index : start_index + maxlen]\n",
    "                print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "                for i in range(400):\n",
    "                    x_pred = np.zeros((1, maxlen, len(unique_tokens)))\n",
    "                    for t, token in enumerate(sentence):\n",
    "                        x_pred[0, t, token_indices[token]] = 1.0\n",
    "                    preds = model.predict(x_pred, verbose=0)[0]\n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_token = indices_tokens[next_index]\n",
    "                    sentence = sentence[1:] + next_token\n",
    "                    generated += next_token\n",
    "\n",
    "                print(\"...Generated: \", generated)\n",
    "                print()\n",
    "    else:\n",
    "        model.fit(x, y, batch_size=batch_size, epochs=epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 206887\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "1453/1617 [=========================>....] - ETA: 5:28 - loss: 5.6974"
     ]
    }
   ],
   "source": [
    "tokens = kant_tokens\n",
    "maxlen=40\n",
    "step=3\n",
    "print_progress=True\n",
    "epochs=40\n",
    "batch_size=128\n",
    "unique_tokens = sorted(list(set(tokens)))\n",
    "token_indices = dict((t, i) for i, t in enumerate(unique_tokens))\n",
    "indices_tokens = dict((i, t) for i, t in enumerate(unique_tokens))\n",
    "sentences = []\n",
    "next_tokens = []\n",
    "for i in range(0, len(tokens)-maxlen, step):\n",
    "    sentences.append(tokens[i : i + maxlen])\n",
    "    next_tokens.append(tokens[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "# i think these are one-hot encoded matrices?\n",
    "x = np.zeros((len(sentences), maxlen, len(unique_tokens)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(unique_tokens)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, token in enumerate(sentence):\n",
    "        x[i, t, token_indices[token]] = 1\n",
    "    y[i, token_indices[next_tokens[i]]] = 1\n",
    "    \n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(unique_tokens))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(unique_tokens), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "if print_progress:\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "        print()\n",
    "        print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(tokens) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print(\"...Diversity:\", diversity)\n",
    "\n",
    "            generated = \"\"\n",
    "            sentence = tokens[start_index : start_index + maxlen]\n",
    "            print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(unique_tokens)))\n",
    "                for t, token in enumerate(sentence):\n",
    "                    x_pred[0, t, token_indices[token]] = 1.0\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_token = indices_tokens[next_index]\n",
    "                sentence = sentence[1:] + next_token\n",
    "                generated += next_token\n",
    "\n",
    "            print(\"...Generated: \", generated)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206887, 40, 20590)\n",
      "(206887, 20590)\n",
      "20590\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 206887\n"
     ]
    }
   ],
   "source": [
    "word_model = train_word_model(kant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
