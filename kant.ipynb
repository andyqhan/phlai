{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "# create cache from scratch\n",
    "# GutenbergCache.create(refresh=False, download=True, unpack=True, parse=True, cache=True, deleteTemp=True)\n",
    "\n",
    "cache = GutenbergCache.get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5637, 28800, 50922, 48433, 5682, 46060, 26585, 5683, 5684, 52821, 4280, 59023]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the cache.query function's kwargs expect lists, like this:\n",
    "cache.query(authors=['Kant, Immanuel'], languages=['en'])\n",
    "# i'm not sure how to make it return the titles too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenbergpy.textget import get_text_by_id, strip_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_text(author):\n",
    "    # wrapper function returning a cleaned and concatenated huge string of all of `author`'s english texts. Doesn't work very well\n",
    "    assert cache\n",
    "    book_ids = cache.query(authors=[author], languages=['en'], downloadtype=['application/plain', 'text/plain'])\n",
    "    corpus = \"\"\n",
    "    for book_id in book_ids:\n",
    "        print(\"concatenating book id \" + str(book_id) + \" ...\")\n",
    "        corpus += str(strip_headers(get_text_by_id(book_id)))\n",
    "    return corpus.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "#from nlpia.loaders import get_data\n",
    "#word_vectors = get_data('wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kant_book_ids = [4280, 5682, 5683, 5684, 46060, 48433, 50922, 52821]\n",
    "kant_book_ids_kingsmill = [5682, 5683, 5684]  # trans. Kingsmill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_books_from_ids(l):\n",
    "    corpus = \"\"\n",
    "    for id in l:\n",
    "        print(\"concatenating book id \" + str(id) + \" ...\")\n",
    "        corpus += str(strip_headers(get_text_by_id(id)))\n",
    "    return str(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    r_space = re.compile(r'((\\\\n)+)|((\\\\x..)+)|((\\\\t)+)')\n",
    "    r_del = re.compile(r\"(\\\\)|'b'|\\\"b'|'b\\\"\")\n",
    "    return re.sub(r_del, '', re.sub(r_space, ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample)\n",
    "        sample_vecs = []\n",
    "        skipped_tokens = set()\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                skipped_tokens.add(token)\n",
    "                pass\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    print(\"skipped tokens:\")\n",
    "    print(skipped_tokens)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating book id 4280 ...\n",
      "concatenating book id 5682 ...\n",
      "concatenating book id 5683 ...\n",
      "concatenating book id 5684 ...\n",
      "concatenating book id 46060 ...\n",
      "concatenating book id 48433 ...\n",
      "concatenating book id 50922 ...\n",
      "concatenating book id 52821 ...\n"
     ]
    }
   ],
   "source": [
    "kant = clean_text(get_books_from_ids(kant_book_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "kant_tokens = tokenizer.tokenize(kant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following tut https://keras.io/examples/generative/lstm_character_level_text_generation/\n",
    "def train_character_model(text, epochs=40, batch_size=128):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    maxlen = 40\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i : i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.GRU(128, input_shape=(maxlen, len(chars))))\n",
    "    model.add(keras.layers.Dense(len(chars)))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    model.summary()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "        print()\n",
    "        print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print(\"...Diversity:\", diversity)\n",
    "\n",
    "            generated = \"\"\n",
    "            sentence = text[start_index : start_index + maxlen]\n",
    "            print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.0\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "                sentence = sentence[1:] + next_char\n",
    "                generated += next_char\n",
    "\n",
    "            print(\"...Generated: \", generated)\n",
    "            print()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1109853\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 128)               82944     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 86)                11094     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 86)                0         \n",
      "=================================================================\n",
      "Total params: 94,038\n",
      "Trainable params: 94,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1109853 samples\n",
      " 600192/1109853 [===============>..............] - ETA: 13:40 - loss: 1.5251"
     ]
    }
   ],
   "source": [
    "char_model = train_character_model(kant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_model(text, print_progress=True, maxlen=40, step=3, epochs=40, batch_size=128):\n",
    "    # wow this is monstrous apparently. An epoch takes an hour to run, and running this as a function crashes Jupyter.\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    unique_tokens = sorted(list(set(tokens)))\n",
    "    token_indices = dict((t, i) for i, t in enumerate(unique_tokens))\n",
    "    indices_tokens = dict((i, t) for i, t in enumerate(unique_tokens))\n",
    "    \n",
    "    sentences = []\n",
    "    next_tokens = []\n",
    "    for i in range(0, len(tokens)-maxlen, step):\n",
    "        sentences.append(tokens[i : i + maxlen])\n",
    "        next_tokens.append(tokens[i + maxlen])\n",
    "    print(\"Number of sequences:\", len(sentences))\n",
    "    \n",
    "    # i think these are one-hot encoded matrices?\n",
    "    x = np.zeros((len(sentences), maxlen, len(unique_tokens)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(unique_tokens)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, token in enumerate(sentence):\n",
    "            x[i, t, token_indices[token]] = 1\n",
    "        y[i, token_indices[next_tokens[i]]] = 1\n",
    "        \n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(maxlen, len(unique_tokens))),\n",
    "            layers.LSTM(128),\n",
    "            layers.Dense(len(unique_tokens), activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    \n",
    "    if print_progress:\n",
    "        for epoch in range(epochs):\n",
    "            model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "            print()\n",
    "            print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "            start_index = random.randint(0, len(tokens) - maxlen - 1)\n",
    "            for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "                print(\"...Diversity:\", diversity)\n",
    "\n",
    "                generated = \"\"\n",
    "                sentence = tokens[start_index : start_index + maxlen]\n",
    "                print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "                for i in range(400):\n",
    "                    x_pred = np.zeros((1, maxlen, len(unique_tokens)))\n",
    "                    for t, token in enumerate(sentence):\n",
    "                        x_pred[0, t, token_indices[token]] = 1.0\n",
    "                    preds = model.predict(x_pred, verbose=0)[0]\n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_token = indices_tokens[next_index]\n",
    "                    sentence = sentence[1:] + next_token\n",
    "                    generated += next_token\n",
    "\n",
    "                print(\"...Generated: \", generated)\n",
    "                print()\n",
    "    else:\n",
    "        model.fit(x, y, batch_size=batch_size, epochs=epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 206887\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "1453/1617 [=========================>....] - ETA: 5:28 - loss: 5.6974"
     ]
    }
   ],
   "source": [
    "tokens = kant_tokens\n",
    "maxlen=40\n",
    "step=3\n",
    "print_progress=True\n",
    "epochs=40\n",
    "batch_size=128\n",
    "unique_tokens = sorted(list(set(tokens)))\n",
    "token_indices = dict((t, i) for i, t in enumerate(unique_tokens))\n",
    "indices_tokens = dict((i, t) for i, t in enumerate(unique_tokens))\n",
    "sentences = []\n",
    "next_tokens = []\n",
    "for i in range(0, len(tokens)-maxlen, step):\n",
    "    sentences.append(tokens[i : i + maxlen])\n",
    "    next_tokens.append(tokens[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "# i think these are one-hot encoded matrices?\n",
    "x = np.zeros((len(sentences), maxlen, len(unique_tokens)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(unique_tokens)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, token in enumerate(sentence):\n",
    "        x[i, t, token_indices[token]] = 1\n",
    "    y[i, token_indices[next_tokens[i]]] = 1\n",
    "    \n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(unique_tokens))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(unique_tokens), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "if print_progress:\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "        print()\n",
    "        print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(tokens) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print(\"...Diversity:\", diversity)\n",
    "\n",
    "            generated = \"\"\n",
    "            sentence = tokens[start_index : start_index + maxlen]\n",
    "            print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(unique_tokens)))\n",
    "                for t, token in enumerate(sentence):\n",
    "                    x_pred[0, t, token_indices[token]] = 1.0\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_token = indices_tokens[next_index]\n",
    "                sentence = sentence[1:] + next_token\n",
    "                generated += next_token\n",
    "\n",
    "            print(\"...Generated: \", generated)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206887, 40, 20590)\n",
      "(206887, 20590)\n",
      "20590\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 206887\n"
     ]
    }
   ],
   "source": [
    "word_model = train_word_model(kant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
